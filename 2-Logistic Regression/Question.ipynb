{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 1: Logistic Regression\n",
    "\n",
    "### Problem Description\n",
    "- Build a logistic regression model to predict whether a student gets admitted into a university based on their scores on 2 exams\n",
    "\n",
    "### Theoretical Knowledge Needed\n",
    "- Logistic regression hypothesis $h_\\theta(x) = g(\\theta^T x)$, where $g(z)$ is the sigmoid function: \n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "- Logistic regression cost function:\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\left[ y^{(i)}\\log(h_\\theta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)})) \\right]$$\n",
    "- Gradient of cost function:\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "### Functions \n",
    "- `sigmoid`: Implements the sigmoid function\n",
    "- `costFunction`: Computes cost and gradient of logistic regression model\n",
    "- `predict`: Generates class predictions given $\\theta$\n",
    "- `plotDecisionBoundary`: Plots decision boundary \n",
    "\n",
    "### Data\n",
    "- `ex2data1.txt`: Training data with 2 exam scores and admission decision for each example\n",
    "\n",
    "### Steps\n",
    "1. Visualize the data\n",
    "2. Implement the `sigmoid` function \n",
    "3. Implement `costFunction` to compute cost and gradient\n",
    "4. Use `fminunc` to learn optimal $\\theta$\n",
    "5. Implement `predict` function to make predictions\n",
    "6. Evaluate classifier accuracy on training set\n",
    "\n",
    "## Exercise 2: Regularized Logistic Regression\n",
    "\n",
    "### Problem Description\n",
    "- Classify microchips as accepted or rejected based on 2 test scores \n",
    "- Use regularized logistic regression to fit non-linear decision boundary\n",
    "\n",
    "### Theoretical Knowledge \n",
    "- Regularized cost function: \n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\left[ y^{(i)}\\log(h_\\theta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)})) \\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2$$\n",
    "- Gradient of regularized cost:\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$ \n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j, \\quad \\text{for } j \\geq 1$$\n",
    "\n",
    "### Functions\n",
    "- `mapFeature`: Maps input features to higher dimensional polynomial features\n",
    "- `costFunctionReg`: Computes regularized cost and gradient\n",
    "\n",
    "### Data \n",
    "- `ex2data2.txt`: Microchip test data \n",
    "\n",
    "### Steps\n",
    "1. Visualize the non-linearly separable training data\n",
    "2. Use `mapFeature` to map features to higher dimensions  \n",
    "3. Implement regularized `costFunction`\n",
    "4. Use `fminunc` to learn $\\theta$\n",
    "5. Plot non-linear decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
